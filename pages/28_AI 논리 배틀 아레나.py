# app.py
# ------------------------------------------------------------
# Streamlit: AI ë…¼ë¦¬ ë°°í‹€ ì•„ë ˆë‚˜ (flash-latest, 1000í† í°, Step3Â·4 ì œê±° + ë‹¤ì‹œ ì‹œë„ ë²„íŠ¼)
# ------------------------------------------------------------
import streamlit as st
import streamlit.components.v1 as components
import re
import json
import requests
from typing import Optional

# ---------------- Page Config ----------------
st.set_page_config(page_title="AI ë…¼ë¦¬ ë°°í‹€ ì•„ë ˆë‚˜", page_icon="ğŸ§ ")

# ---------------- ì„¸ì…˜ ì´ˆê¸°í™” ----------------
def init_session_state():
    if "student_claim" not in st.session_state:
        st.session_state.student_claim = ""
    if "ai_response" not in st.session_state:
        st.session_state.ai_response = ""
    if "student_followup_answer" not in st.session_state:
        st.session_state.student_followup_answer = ""
    if "final_evaluation" not in st.session_state:
        st.session_state.final_evaluation = ""
    if "ai_response_json" not in st.session_state:
        st.session_state.ai_response_json = {}
    # ì‚¬ìš©ìê°€ ì§ì ‘ ì…ë ¥í•œ API í‚¤(ì„¸ì…˜ ë‹¨ìœ„ ì €ì¥)
    if "user_gpt_key" not in st.session_state:
        st.session_state.user_gpt_key = ""


def _extract_followup_question(ai_text: str) -> str:
    """AIì˜ ì‘ë‹µì—ì„œ 'í•™ìƒì´ ë˜ì§ˆ ìˆ˜ ìˆëŠ” ë˜ë¬»ëŠ” ì§ˆë¬¸'ì„ ì¶”ì¶œí•©ë‹ˆë‹¤. ë°œê²¬ë˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ ì§ˆë¬¸ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if not ai_text:
        return "ì´ ë°˜ë°•ì— ëŒ€í•´ ì–´ë–»ê²Œ ë” ì§ˆë¬¸í•˜ì‹œê² ìŠµë‹ˆê¹Œ?"

    # ìš°ì„ : ëª…ì‹œì  íƒœê·¸ <FOLLOWUP_QUESTION>...</FOLLOWUP_QUESTION> ê²€ìƒ‰ (ë©€í‹°ë¼ì¸ ìº¡ì²˜)
    tag_match = re.search(r"<FOLLOWUP_QUESTION>([\s\S]+?)</FOLLOWUP_QUESTION>", ai_text, re.IGNORECASE)
    if tag_match:
        q = tag_match.group(1).strip()
        # ì§ˆë¬¸í˜•ìœ¼ë¡œ ëë‚˜ì§€ ì•Šìœ¼ë©´ ë¬¼ìŒí‘œ ì¶”ê°€
        if not q.endswith("?"):
            q = q.rstrip('.') + "?"
        return q

    # ë¼ë²¨ì„ ê²€ìƒ‰ (ê¸°ì¡´ ë¡œì§)
    m = re.search(r"í•™ìƒ[\s\S]{0,10}ë˜ë¬»ëŠ” ì§ˆë¬¸[:ï¼š]?\s*(.+)$", ai_text, re.MULTILINE)
    if m:
        candidate = m.group(1).strip()
        first_line = candidate.splitlines()[0].strip()
        return first_line

    parts = ai_text.splitlines()
    for i, line in enumerate(parts):
        if "ë˜ë¬»" in line or "ë˜ë¬»ëŠ”" in line or "ì§ˆë¬¸" in line:
            for j in range(i+1, min(i+6, len(parts))):
                maybe = parts[j].strip()
                if maybe:
                    return maybe

    # ë¬¼ìŒí‘œê°€ í¬í•¨ëœ ë§ˆì§€ë§‰ ë¬¸ì¥
    sentences = re.split(r"(?<=[.?!])\s+", ai_text)
    for s in reversed(sentences):
        if s.strip().endswith("?"):
            return s.strip()

    return "ì´ ë°˜ë°•ì— ëŒ€í•´ ì–´ë–»ê²Œ ë” ì§ˆë¬¸í•˜ì‹œê² ìŠµë‹ˆê¹Œ?"


def call_gemini_evaluate(claim: str, ai_response: str, student_answer: str) -> Optional[str]:
    """í•™ìƒì˜ ë‹µë³€ì— ëŒ€í•´ AIê°€ ìµœì¢… í‰ê°€ì™€ ì¡°ì–¸ì„ ì œê³µí•˜ë„ë¡ Geminiì— ìš”ì²­í•©ë‹ˆë‹¤."""
    # ìš°ì„  ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì„¸ì…˜ í‚¤ë¥¼ ì‚¬ìš©í•˜ê³ , ì—†ìœ¼ë©´ Streamlit secretsì˜ í‚¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    # ìš°ì„  ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì„¸ì…˜ í‚¤ë¥¼ ì‚¬ìš©í•˜ê³ , ì—†ìœ¼ë©´ Streamlit secretsì˜ OPENAI_API_KEY, ê·¸ë‹¤ìŒ GOOGLE_API_KEYë¡œ í´ë°±
    api_key = (
        st.session_state.get("user_gpt_key")
        or st.secrets.get("OPENAI_API_KEY")
        or st.secrets.get("GOOGLE_API_KEY")
    )
    if not api_key:
        st.error("API í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í˜ì´ì§€ ìƒë‹¨ì—ì„œ API í‚¤ë¥¼ ì…ë ¥í•˜ê±°ë‚˜ Streamlit secretsì— ì„¤ì •í•˜ì„¸ìš”.")
        return None

    # OpenAI Chat Completions í˜¸ì¶œ helper
    def _call_openai_chat(system: str, user: str, model: str = "gpt-3.5-turbo", max_tokens: int = 1024, temperature: float = 0.6) -> Optional[str]:
        url = "https://api.openai.com/v1/chat/completions"
        headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
        payload = {
            "model": model,
            "messages": [
                {"role": "system", "content": system},
                {"role": "user", "content": user},
            ],
            "max_tokens": max_tokens,
            "temperature": temperature,
        }
        try:
            r = requests.post(url, headers=headers, json=payload, timeout=30)
            r.raise_for_status()
            data = r.json()
            # í‘œì¤€ ì‘ë‹µ êµ¬ì¡°ì—ì„œ ì²« ë²ˆì§¸ ì„ íƒì§€ì˜ ë©”ì‹œì§€ í…ìŠ¤íŠ¸ ë°˜í™˜
            choices = data.get("choices") or []
            if choices:
                msg = choices[0].get("message", {}).get("content")
                return msg
            return None
        except Exception as e:
            st.error(f"OpenAI í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return None

    prompt = f"""
ë‹¹ì‹ ì€ ê³ ë“±í•™êµ ìˆ˜í•™ ìˆ˜ì—…ì˜ ë…¼ë¦¬ í† ë¡  íŒŒíŠ¸ë„ˆì…ë‹ˆë‹¤.
í•™ìƒì˜ ì›ë˜ ì£¼ì¥: "{claim}"
AIì˜ ë°˜ë°•(ìš”ì•½):
{ai_response}

í•™ìƒì˜ ë‹µë³€(í•™ìƒì´ AIì˜ ë°˜ë°•ì— ëŒ€í•´ ëŒ€ë‹µí•œ ë‚´ìš©):
{student_answer}

ìœ„ì˜ í•™ìƒ ë‹µë³€ì„ í‰ê°€í•˜ê³ , ë‹¤ìŒì„ í¬í•¨í•œ ìµœì¢… í‰ê°€ì™€ êµ¬ì²´ì ì¸ ì§€ë„(ì¡°ì–¸)ë¥¼ ì œì‹œí•˜ì„¸ìš”:
1) í•™ìƒ ë‹µë³€ì˜ ê°•ì ê³¼ ì•½ì  (ê°„ë‹¨íˆ)
2) ë…¼ë¦¬ì  ì˜¤ë¥˜ë‚˜ ì˜¤í•´ê°€ ìˆë‹¤ë©´ ì§€ì 
3) ë‹¤ìŒ í† ë¡ ì—ì„œ í•™ìƒì´ ë” ë°œì „ì‹œí‚¬ ìˆ˜ ìˆëŠ” êµ¬ì²´ì ì¸ ì—°ìŠµì´ë‚˜ ì§ˆë¬¸ ì¶”ì²œ

ì¶œë ¥ì€ ëª…í™•í•œ ë¬¸ë‹¨ìœ¼ë¡œ ì œê³µí•˜ì„¸ìš”.
"""

    # ìš”ì•½ ìš”ì²­ì„ í•¨ê»˜ í¬í•¨ì‹œì¼œ OpenAIì— ì „ë‹¬
    user_prompt = prompt + "\nìš”ì²­: ì•„ë˜ ì¶œë ¥ì€ ê°„ê²°í•˜ê²Œ (6ë¬¸ì¥ ì´ë‚´ ë˜ëŠ” ì•½ 180ë‹¨ì–´ ì´ë‚´) ìš”ì•½í•´ ì£¼ì„¸ìš”.\n"
    return _call_openai_chat(system="You are a helpful assistant for evaluating student answers.", user=user_prompt, model="gpt-3.5-turbo", max_tokens=1024, temperature=0.6)


# ---------------- Gemini í˜¸ì¶œ í•¨ìˆ˜ ----------------
def call_gemini(claim: str) -> Optional[str]:
    """Gemini AI í˜¸ì¶œ (flash-latest, 1000í† í° ê³ ì •)"""
    # ìš°ì„  ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì„¸ì…˜ í‚¤ë¥¼ ì‚¬ìš©í•˜ê³ , ì—†ìœ¼ë©´ Streamlit secretsì˜ í‚¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    api_key = (
        st.session_state.get("user_gpt_key")
        or st.secrets.get("OPENAI_API_KEY")
        or st.secrets.get("GOOGLE_API_KEY")
    )
    if not api_key:
        st.error("API í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í˜ì´ì§€ ìƒë‹¨ì—ì„œ API í‚¤ë¥¼ ì…ë ¥í•˜ê±°ë‚˜ Streamlit secretsì— ì„¤ì •í•˜ì„¸ìš”.")
        return None
    # OpenAI Chat Completions í˜¸ì¶œ helper (ì¬ì‚¬ìš©)
    def _call_openai_chat(system: str, user: str, model: str = "gpt-3.5-turbo", max_tokens: int = 1500, temperature: float = 0.7) -> Optional[str]:
        url = "https://api.openai.com/v1/chat/completions"
        headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
        payload = {
            "model": model,
            "messages": [
                {"role": "system", "content": system},
                {"role": "user", "content": user},
            ],
            "max_tokens": max_tokens,
            "temperature": temperature,
        }
        try:
            r = requests.post(url, headers=headers, json=payload, timeout=30)
            r.raise_for_status()
            data = r.json()
            choices = data.get("choices") or []
            if choices:
                return choices[0].get("message", {}).get("content")
            return None
        except Exception as e:
            st.error(f"OpenAI í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return None

    # JSON í˜•ì‹ìœ¼ë¡œ ì •í™•íˆ ì¶œë ¥í•˜ë„ë¡ ëª¨ë¸ì— ê°•í•˜ê²Œ ì§€ì‹œí•©ë‹ˆë‹¤.
    prompt = f"""
ë‹¹ì‹ ì€ ê³ ë“±í•™êµ ìˆ˜í•™ ìˆ˜ì—…ì˜ ë…¼ë¦¬ í† ë¡  íŒŒíŠ¸ë„ˆì…ë‹ˆë‹¤.
í•™ìƒì˜ ì£¼ì¥: "{claim}"

ì´ ì£¼ì¥ì— ëŒ€í•´ ìœ„ì˜ ìš”êµ¬ì‚¬í•­ì— ë”°ë¼ ë…¼ë¦¬ì ìœ¼ë¡œ ë°˜ë°•í•œ ë’¤, ì•„ë˜ì˜ JSON ê°ì²´ í•˜ë‚˜ë§Œì„ ì¶œë ¥í•˜ì„¸ìš”.
JSON í‚¤ëŠ” ì •í™•íˆ ë‹¤ìŒê³¼ ê°™ì•„ì•¼ í•©ë‹ˆë‹¤:
{{
    "error_type": "(ì˜¤ë¥˜ ìœ í˜•)",
    "definition_explanation": "(ì •ì˜ì— ê·¼ê±°í•œ ì„¤ëª…)",
    "counterexamples": ["(ë°˜ë¡€1)", "(ë°˜ë¡€2)"],
    "one_line_conclusion": "(í•œì¤„ ê²°ë¡ )",
    "followup_question": "(í•™ìƒì´ ë˜ì§ˆ ìˆ˜ ìˆëŠ” ë˜ë¬»ëŠ” ì§ˆë¬¸, ë°˜ë“œì‹œ ë¬¼ìŒí‘œë¡œ ëë‚  ê²ƒ)"
}}

ì¤‘ìš”: ì¶œë ¥ì€ ë°˜ë“œì‹œ ìœ„ì˜ JSON ê°ì²´ë§Œ ë‹¨ë…ìœ¼ë¡œ ë°˜í™˜í•˜ê³ , ì¶”ê°€ ì„¤ëª…ì´ë‚˜ í…ìŠ¤íŠ¸ëŠ” ì²¨ê°€í•˜ì§€ ë§ˆì„¸ìš”. followup_question ê°’ì€ ë°˜ë“œì‹œ ë¬¼ìŒí‘œ(?)ë¡œ ëë‚˜ì•¼ í•©ë‹ˆë‹¤.

ì¶”ê°€ ìš”êµ¬ì‚¬í•­: `error_type` í•„ë“œëŠ” í•œêµ­ì–´ë¡œ ê°„ë‹¨íˆ ì‘ì„±í•˜ê³ , í•„ìš”í•˜ë©´ ê´„í˜¸ ì•ˆì— ì˜ì–´ ì›ë¬¸ì„ ë§ë¶™ì—¬ ì£¼ì„¸ìš”. ì˜ˆ: "ë‹¨ì¼ ì›ì¸ ì˜¤ë¥˜ (Single Cause Fallacy)".
"""

    # OpenAIë¡œ í˜¸ì¶œ
    text = _call_openai_chat(system="You are a helpful assistant that produces a single JSON object answer.", user=prompt, model="gpt-3.5-turbo", max_tokens=1500, temperature=0.7)
    if not text:
        st.warning("AI ì‘ë‹µì´ ë¹„ì–´ ìˆê±°ë‚˜ ì°¨ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return None

    # ì‘ë‹µì—ì„œ JSON ê°ì²´ ì¶”ì¶œ ì‹œë„
    try:
        jstart = text.find("{")
        jend = text.rfind("}")
        if jstart != -1 and jend != -1 and jend > jstart:
            json_text = text[jstart:jend+1]
            parsed = json.loads(json_text)
            # followup_questionì´ ë¬¼ìŒí‘œë¡œ ëë‚˜ëŠ”ì§€ ë³´ì¥
            fq = parsed.get("followup_question")
            if fq and not fq.strip().endswith("?"):
                parsed["followup_question"] = fq.strip().rstrip('.') + "?"
            # ì €ì¥
            st.session_state.ai_response_json = parsed
            # ì‚¬ëŒì´ ë³¼ ìˆ˜ ìˆë„ë¡ ì›ë³¸ í…ìŠ¤íŠ¸ë„ ë°˜í™˜
            return text
    except Exception:
        # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì›ë³¸ í…ìŠ¤íŠ¸ ë°˜í™˜
        st.session_state.ai_response_json = {}
        return text

    st.warning("AI ì‘ë‹µì´ ë¹„ì–´ ìˆê±°ë‚˜ ì°¨ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    return None


def _format_error_type(raw: str) -> str:
    """ê°€ëŠ¥í•˜ë©´ í•œêµ­ì–´(ì˜ì–´ ë³‘ê¸°) í˜•íƒœë¡œ í¬ë§·í•©ë‹ˆë‹¤. ì´ë¯¸ í•œêµ­ì–´ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜."""
    if not raw:
        return ""
    # ì´ë¯¸ í•œê¸€ì„ í¬í•¨í•˜ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
    if re.search(r"[\u3131-\u318E\uAC00-\uD7A3]", raw):
        return raw
    # ê°„ë‹¨í•œ ë§¤í•‘
    mapping = {
        "Ignoring Multiple Causality": "ë‹¤ì¤‘ ì›ì¸ ë¬´ì‹œ",
        "Multiple Sufficient Conditions": "ë‹¤ì¤‘ ì¶©ë¶„ ì›ì¸",
        "Single Cause Fallacy": "ë‹¨ì¼ ì›ì¸ ì˜¤ë¥˜",
        "Ignoring Multiple Causes": "ë‹¤ì¤‘ ì›ì¸ ë¬´ì‹œ",
        "Hasty Generalization": "ì„±ê¸‰í•œ ì¼ë°˜í™”",
    }
    # ì˜ì–´ í‚¤ì— ëŒ€í•´ ë§¤í•‘ì„ ì°¾ìŒ
    for eng, kor in mapping.items():
        if eng.lower() in raw.lower():
            # ì›ë¬¸ë„ ë³‘ê¸°
            return f"{kor} ({eng})"
    # ê¸°ë³¸: ì˜ì–´ ì›ë¬¸ì„ ê´„í˜¸ë¡œ ì œê³µ
    return raw

# ---------------- ë©”ì¸ ì•± ----------------
init_session_state()
st.header("AI ë…¼ë¦¬ ë°°í‹€ ì•„ë ˆë‚˜ ğŸ§ âš”ï¸")

try:
    st.caption("OpenAI Chat API ì‚¬ìš©: ëª¨ë¸ gpt-3.5-turbo (HTTP í˜¸ì¶œ)")
except Exception:
    pass

# --- ì‚¬ìš©ì ì…ë ¥í˜• API í‚¤ ---
with st.expander("ğŸ” API Key ì„¤ì • (ì§ì ‘ ì…ë ¥)", expanded=False):
    st.markdown("ì—¬ê¸°ì— ë³¸ì¸ì˜ ChatGPT API Keyë¥¼ ì…ë ¥í•˜ì„¸ìš”. ì…ë ¥í•œ í‚¤ëŠ” ì´ ì„¸ì…˜ì—ë§Œ ì €ì¥ë©ë‹ˆë‹¤.")
    # ì„¸ì…˜ ìƒíƒœ í‚¤ì™€ ë™ì¼í•œ keyë¥¼ ì‚¬ìš©í•˜ë©´ ìë™ìœ¼ë¡œ st.session_stateì— ë°˜ì˜ë©ë‹ˆë‹¤.
    st.text_input(
        "Google API Key (Geminiìš©)",
        value=st.session_state.user_gpt_key,
        placeholder="ì˜ˆ: ya29.... ë˜ëŠ” sk-...",
        type="password",
        key="user_gpt_key",
    )
    st.caption("â€» í‚¤ë¥¼ ë¹ˆì¹¸ìœ¼ë¡œ ë‘ê±°ë‚˜ ì˜¬ë°”ë¥¸ í‚¤ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë©´ í˜¸ì¶œì´ ì°¨ë‹¨ë©ë‹ˆë‹¤.")

# Step 1
st.subheader("Step 1: AIì—ê²Œ ë„ì „ì¥ ë‚´ë°€ê¸°")
# ëª¨ë‘  ì´ë¦„ ì…ë ¥ì°½ ì œê±°(ìš”ì²­ì— ë”°ë¼)
st.session_state.student_claim = st.text_area(
    "ì˜¤ë¥˜ê°€ í¬í•¨ëœ ëª…ì œ ì…ë ¥",
    value=st.session_state.student_claim,
    placeholder="ì˜ˆ: AIê°€ ì‚¬ëŒë³´ë‹¤ ë” ë¹ ë¥´ê²Œ í•™ìŠµí•˜ê³  ë¬¸ì œë¥¼ í‘¸ëŠ” ì‹œëŒ€ê°€ ì™”ìœ¼ë‹ˆ, ì¸ê°„ì˜ ì°½ì˜ë ¥ì€ ì´ë¯¸ ì¸ê³µì§€ëŠ¥ì—ê²Œ ì™„ì „íˆ ë’¤ì²˜ì¡Œë‹¤.",
    height=120,
)

# Step 2
st.subheader("Step 2: AIì˜ ë°˜ë°• í™•ì¸í•˜ê¸°")
col1, col2 = st.columns(2)

with col1:
    if st.button("ğŸ¤– AI, ë‚´ ì£¼ì¥ì„ ë°˜ë°•í•´ë´!"):
        if not st.session_state.student_claim.strip():
            st.warning("ë¨¼ì € ëª…ì œë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        else:
            with st.spinner("AIê°€ ë°˜ë°•ì„ ì¤€ë¹„í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
                ai_out = call_gemini(st.session_state.student_claim)
                if ai_out:
                    st.session_state.ai_response = ai_out
    # ë²„íŠ¼ ë°”ë¡œ ì•„ë˜ì— ê°„ë‹¨í•œ ìƒˆë¡œê³ ì¹¨ ì•ˆë‚´(ì•„ì´ì½˜ + êµµì€ ë¹¨ê°„ìƒ‰)
    st.markdown('<span style="color: red; font-weight: 700;">ğŸ”&nbsp;ìƒˆë¡œê³ ì¹¨:&nbsp;&nbsp;Ctrl+R</span>', unsafe_allow_html=True)

with col2:
    # ì˜¤ë¥¸ìª½ ì¹¸ì˜ ì•ˆë‚´ë¬¸ì€ ì œê±°(ìœ„ì¹˜ ì´ë™)
    st.write("")


# ë°˜ë°• ì¶œë ¥
if st.session_state.ai_response:
    st.markdown("### ğŸ¤– AIì˜ ë°˜ë°• ê²°ê³¼")
    # JSONìœ¼ë¡œ íŒŒì‹±ëœ êµ¬ì¡°í™” ì‘ë‹µì´ ìˆìœ¼ë©´ êµ¬ì¡°í™”í•˜ì—¬ í‘œì‹œ
    if st.session_state.get("ai_response_json"):
        parsed = st.session_state.ai_response_json
        err_raw = parsed.get('error_type','')
        st.markdown(f"**ì˜¤ë¥˜ ìœ í˜•:** {_format_error_type(err_raw)}")
        st.markdown("**ì •ì˜ì— ê·¼ê±°í•œ ì„¤ëª…:**")
        st.write(parsed.get('definition_explanation',''))
        st.markdown("**ë°˜ë¡€:**")
        for ce in parsed.get('counterexamples', []) or []:
            st.write(f"- {ce}")
        st.markdown("**í•œì¤„ ê²°ë¡ :**")
        st.write(parsed.get('one_line_conclusion',''))

        followup_q = parsed.get('followup_question', '')
        if followup_q and not followup_q.strip().endswith('?'):
            followup_q = followup_q.strip().rstrip('.') + '?'

        st.markdown("**AIê°€ ì œì‹œí•œ ì§ˆë¬¸ (í•™ìƒì´ ì‘ë‹µí•´ ë³´ì„¸ìš”):**")
        st.info(followup_q or "ì´ ë°˜ë°•ì— ëŒ€í•´ ì–´ë–»ê²Œ ì§ˆë¬¸í•˜ì‹œê² ìŠµë‹ˆê¹Œ?")
    else:
        # íŒŒì‹±ëœ JSONì´ ì—†ìœ¼ë©´ ê¸°ì¡´ ë™ì‘(ì›ë¬¸ í‘œì‹œ ë° ì¶”ì¶œê¸° ì‚¬ìš©)
        st.markdown(st.session_state.ai_response)
        followup_q = _extract_followup_question(st.session_state.ai_response)
        st.markdown("**AIê°€ ì œì‹œí•œ ì§ˆë¬¸ (í•™ìƒì´ ì‘ë‹µí•´ ë³´ì„¸ìš”):**")
        st.info(followup_q)

    # í•™ìƒì´ ê·¸ ì§ˆë¬¸ì— ë‹µí•  ìˆ˜ ìˆëŠ” ì…ë ¥ì°½
    st.session_state.student_followup_answer = st.text_area(
        "í•™ìƒì˜ ë‹µë³€",
        value=st.session_state.student_followup_answer,
        placeholder="ì—¬ê¸°ì— í•™ìƒì´ AI ì§ˆë¬¸ì— ë‹µí•œ ë‚´ìš©ì„ ì…ë ¥í•˜ì„¸ìš”.",
        height=120,
    )

    if st.button("ğŸ“˜ AIì˜ ìµœì¢… í‰ê°€ ë° ì¡°ì–¸ ë°›ê¸°"):
        if not st.session_state.student_followup_answer.strip():
            st.warning("ë¨¼ì € í•™ìƒì˜ ë‹µë³€ì„ ì…ë ¥í•˜ì„¸ìš”.")
        else:
            with st.spinner("AIê°€ ìµœì¢… í‰ê°€ì™€ ì¡°ì–¸ì„ ì‘ì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
                eval_out = call_gemini_evaluate(
                    st.session_state.student_claim,
                    st.session_state.ai_response,
                    st.session_state.student_followup_answer,
                )
                if eval_out:
                    st.session_state.final_evaluation = eval_out

    if st.session_state.final_evaluation:
        st.markdown("### ğŸ“ AIì˜ ìµœì¢… í‰ê°€ ë° ì¡°ì–¸")
        st.markdown(st.session_state.final_evaluation)